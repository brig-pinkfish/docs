---
title: "Scraping Commands Guide"
description: "Learn how to use scraper skill in your automations"
---

## Introduction

The scraper allows you to navigate to a particular location in a website and then bring back the contents. In the simplest case, you give the scraper a URL and it gives you back the contents of that page as text (and screenshot). Using actions, you can login, navigate menus, and even loop through filling out forms with data you've collected in a previous step. You can also use AI Vision to answer questions about content on the page or to scrape and structure data all in one request.

## Table of Contents

- [Basic Scraping](#basic-scraping)
- [Available Commands](#available-commands)
- [Advanced Features](#advanced-features)
- [Common Examples](#common-examples)

## Basic Scraping

Scrape a web page with the `/scraper` command followed by one or more URLs:

```
/scraper
url: https://example.com
```

Scrape and specify the return format:

```
/scraper
url: https://example.com
format: markdown
```

### Format Options

- `format: markdown` - Returns content in readable text format (default)
- `format: html` - Returns HTML content

### Content Filtering

- `include: article, .main-content, #content` - Specify elements to include
- `exclude: nav, .sidebar, #footer` - Specify elements to exclude
- `mainContent: false` - Include all content (default is true, which focuses on main content)

## Available Actions

Use actions when you need to navigate to a certain location before doing your scrape. Vision is a bit of an outlier in the action list b/c it's an altnerate method of scraping (versus a navigation action).

**Action Types**

1. **Navigation Actions**

   - `goTo: [URL]` - Navigate to a specific page
   - `wait: [milliseconds]` - Pause for specified time
   - `scrollBottom` - Scroll to bottom of page
   - `scrollTo: [selector]` - Scroll to specific element

2. **Interactive Actions**

   - `click: [text or selector]` - Click on an element
   - `hover: [text or selector]` - Move mouse over element
   - `fill: [field] with [text]` - Enter text into input field
   - `select: [option]` - Choose from dropdown menu
   - `check: [checkbox]` - Toggle checkbox/radio button
   - `press: [key]` - Press keyboard key (Enter, Tab, etc.)

3. **Advanced Actions**
   - `screenshot: [full page or selector]` - Capture screenshot
   - `vision: [prompt]` - Use AI to analyze page content
   - `runcode: [Playwright code]` - Execute custom PW code
   - `loop: [actions]` - Repeat actions with different inputs

### Waiting

Waiting between actions is a common pattern in scraping and often necessary. You can use the `wait` action to pause for a specified amount of time. So if something isn't working, try adding a wait in between actions.

### Examples

**Basic Navigation:**

```
/scraper
goTo: https://example.com
wait: 2000
click: "Accept Cookies"
scrollBottom
screenshot: full page
```

**Form Filling:**

```
/scraper
goTo: https://login.example.com
fill: Username with "myuser"
fill: Password with "mypass"
click: "Login"
wait: 1000
screenshot
```

**Working with Lists:**

```
/scraper
goTo: https://shop.example.com
select: "Sort by Price"
wait: 500
vision: "Extract all product prices as a table"
```

## Advanced Features

**Loop Example:**

```
/scraper
goTo: https://shop.example.com
loop begin:
  - click: "Add to Cart" in .product-card
  - wait: 1000
loop end
```

**Loop with Data Example:**
First assemble data into an array of objects. You might do this in a previous step and then refer to the output of that step. Let's say that your data output for Step 1 looks like this:

```
[
   {"color":"#ff0000","datetime":"2024-12-21T08:30","email":"test@pinkfish.ai","month":"2026-12","number":"22"},
   {"color":"#00ff00","datetime":"2023-12-21T08:30","email":"hello@pinkfish.ai","month":"2025-12","number":"23"},
   {"color":"#0000ff","datetime":"2022-12-21T08:30","email":"dev@pinkfish.ai","month":"2024-02","number":"24"}
]
```

Then you would write your scraper in Step 2 like this:

```
Using the result from the previous step as inputData for the loop:
/scraper
* goto https://testpages.eviltester.com/styled/html5-form-test.html
loop begin
   - fill color input with "color" data
   - fill datetime-local input with "datetime" data
   - fill email input with "email" data
   - fill month input with "month" data
   - fill number input with "number" data
   - click submit button
   - wait 5 secs
loop end
* also wait 2 secs between each action
```

### Using Secrets

You can use secrets that you've stored in the datastore in combination with scraping to fill in username, password, and apikey fields in order to avoid entering in your credentials into your automation in plain text.

Assuming you've previously stored a secret in the datastore called "salesforceLogin", you could run the scraper with your secret as follows:

```
/get-secret salesforceLogin
/scraper
* goto https://mycompany.lightning.force.com/lightning/page/home
- fill username input with usename from secret
- fill password input with password from secret
- wait 5 secs
```

### Parent Scoping

Specify a parent container to narrow down element selection:

```
/scraper
click: "Submit" in .form-container
```

### AI Vision

Use Vision AI to extract specific information:

```
/scraper
goTo: https://shop.example.com
vision: "Give me all products under $50 and list their features"
```

### Batch Processing

Process multiple pages at once:

```
/scraper
urls:
  - https://example.com/page1
  - https://example.com/page2
format: markdown
wait: 1000
screenshot: full page
```

## Common Examples

1. **Login and Extract Content**

```
/get-secret ExampleComLogin
/scraper
goTo: https://www.example.com
fill: Username with username from secret
fill: Password with password from secret
click: "Login"
wait: 2000
vision: "Give me all dashboard statistics"
```

2. **Shopping Cart Process**

```
/scraper
goTo: https://shop.example.com
select: "Sort by Price: Low to High"
wait: 1000
click: "Add to Cart" in .first-product
click: "Checkout"
screenshot: full page
```

3. **Content Analysis**

```
/scraper
goTo: https://blog.example.com
scrollBottom
vision: "Summarize the main points of the article"
screenshot
```

4. **Form Submission**

```
/scraper
goTo: https://forms.example.com
fill: Name with "John Doe"
fill: Email with "john@example.com"
check: "Accept Terms"
click: "Submit"
wait: 2000
screenshot
```

## Selector Types

When targeting elements on a page, you can use several different methods:

1. **By Text Content**

   - `click: "Login"` - Clicks element containing exact text "Login"
   - `click: "Sign up for free"` - Works with longer phrases too

2. **By Placeholder Text**

   - `fill: "Search..." with "laptops"` - Targets input field with placeholder text
   - `click: "Enter your username"` - Works with any placeholder text

3. **CSS Selectors**

   - `click: #login-button` - Using ID (#)
   - `click: .submit-btn` - Using class name (.)
   - `click: button.primary` - Element type with class
   - `click: .form > .submit` - Using hierarchy
   - `click: [data-testid="submit"]` - Using attributes

4. **XPath Selectors**

   - `click: //button[@type="submit"]` - Button with specific type
   - `click: //div[@class="menu"]//a` - Links within menu div
   - `click: //h1[contains(text(), "Welcome")]` - Heading containing text
   - `click: //label[text()="Password"]/following-sibling::input` - Input field after label
   - `click: //*[@id="main"]//button[2]` - Second button in main container

5. **Combined Approaches**
   - `click: "Submit" in .form-container` - Text within specific container
   - `click: "Login" in #auth-modal` - Text within ID
   - `hover: "Menu" in .nav-bar` - Combining text and class
   - `click: "Next" in //div[@class="pagination"]` - Text within XPath-selected container

Remember:

- Text matching is case-sensitive
- For elements with spaces in selectors, use quotes: `".main content"`
- When using text, prefer exact matches over partial ones
- CSS selectors provide simple targeting for basic needs
- XPath provides more powerful selection capabilities:
  - Can traverse up the DOM tree
  - Can select elements based on their contents
  - Supports complex relationships between elements
- Always try to use the most specific selector that works

Happy scraping!
