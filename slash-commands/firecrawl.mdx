---
title: "Firecrawl Command Guide"
description: "Learn how to use the Firecrawl slash command to scrape and map websites"
---

## What can you do with it?

Extract clean, structured content from websites including text, links, screenshots, and metadata. Perfect for content extraction, web monitoring, and data collection with browser automation support.

## How to use it?

### Basic Command Structure
```
/firecrawl
urls: the web pages to scrape
format: how you want the content (markdown by default)
```

### Parameters
- **urls**: Array of URLs to scrape
- **format** (optional): Output format - markdown, html, links, screenshot, json
- **actions** (optional): Browser actions like scroll, click, wait, screenshot
- **wait time** (optional): Milliseconds to wait before scraping
- **only main content** (optional): Extract only the main content area
- **include tags** (optional): Specific HTML tags to include
- **exclude tags** (optional): HTML tags to exclude

### Response Format
Returns scraped content in the requested format with metadata including title, language, and status.

## Examples

### Basic Usage
Scrape a webpage as markdown:
```
/firecrawl
urls: ["https://example.com"]
```

### Advanced Usage
Scrape with browser actions:
```
/firecrawl
urls: ["https://example.com/products"]
actions: scroll down, wait 2 seconds, take screenshot
format: markdown and screenshot
```

### Specific Use Case
Extract only main article content:
```
/firecrawl
urls: ["https://news.example.com/article"]
only main content: true
exclude tags: ["script", "style", "nav", "footer"]
```

## Key Features

### Content Extraction
- Scrape multiple URLs
- Extract main content
- Remove unwanted elements
- Clean HTML output
- Convert to markdown

### Output Formats
- Markdown (default)
- HTML (processed)
- Raw HTML
- JSON structure
- Screenshots
- Link extraction

### Browser Actions
- Take screenshots
- Scroll pages
- Wait for content
- Click elements
- Full page capture

### Content Filtering
- Include specific tags
- Exclude unwanted tags
- Main content only
- Remove base64 images

## Example Commands

### Basic Scrape
```
/firecrawl scrape https://example.com
```

### Multiple URLs
```
/firecrawl scrape https://site1.com and https://site2.com as markdown
```

### With Screenshot
```
/firecrawl capture https://example.com with full page screenshot
```

### Extract Links
```
/firecrawl get all links from https://example.com
```

### Custom Tags
```
/firecrawl scrape https://example.com including only div, p, h1, h2 tags
```

## Configuration Options

### Output Formats
- `markdown`: Clean markdown content
- `html`: Processed HTML
- `rawHtml`: Original HTML
- `links`: Array of links
- `screenshot`: Base64 image
- `json`: Structured data

### Browser Actions
```json
{
  "type": "screenshot",
  "fullPage": true
}
```

```json
{
  "type": "scroll",
  "direction": "down"
}
```

### Wait Options
- Default: 1000ms
- Custom: specify milliseconds
- Ensures content loads

## Tag Filtering

### Include Tags
- `div`, `p`, `h1`, `h2`
- Article content tags
- Custom selections

### Exclude Tags
- `script`, `style`, `noscript`
- Ads and tracking
- Unwanted elements

## Response Data

### Success Response
- Scraped content
- Metadata (title, language)
- Source URL
- Status code
- File URLs

### Metadata Includes
- Page title
- Language
- Referrer
- Scrape ID
- Status code

## Tips
- Use markdown format for clean text
- Enable screenshots for visual content
- Filter tags for cleaner output
- Set appropriate wait times for dynamic content 