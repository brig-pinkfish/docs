---
title: "Firecrawl Command Guide"
description: "Learn how to use the Firecrawl slash command to scrape and map websites"
---

## What can you do with it?

Extract clean, structured content from websites including text, links, screenshots, and metadata. Perfect for content extraction, web monitoring, and data collection with browser automation support.

## How to use it?

### Basic Command Structure
```
/firecrawl [urls] [format]
```

### Parameters

**Required:**

- `urls` - Array of URLs to scrape

**Optional:**

- `format` - Output format: markdown, html, links, screenshot, json (default: markdown)

- `actions` - Browser actions like scroll, click, wait, screenshot

- `wait time` - Milliseconds to wait before scraping

- `only main content` - Extract only the main content area

- `include tags` - Specific HTML tags to include

- `exclude tags` - HTML tags to exclude

### Response Format

The command returns:
```json
{
  "content": "scraped content in requested format",
  "metadata": {
    "title": "page title",
    "language": "detected language",
    "status": "success or error"
  }
}
```

## Examples

### Basic Usage
```
/firecrawl
urls: ["https://example.com"]
```
Scrape a webpage and return content as markdown.

### Advanced Usage
```
/firecrawl
urls: ["https://example.com/products"]
actions: scroll down, wait 2 seconds, take screenshot
format: markdown and screenshot
```
Scrape with browser automation actions and capture screenshots.

### Specific Use Case
```
/firecrawl
urls: ["https://news.example.com/article"]
only main content: true
exclude tags: ["script", "style", "nav", "footer"]
```
Extract only the main article content while excluding navigation and footer elements.

### Notes

Supports multiple output formats including markdown (default), HTML, raw HTML, JSON, screenshots, and link extraction. Browser actions available include screenshot, scroll, wait, and click. Tag filtering allows including or excluding specific HTML elements. Default wait time is 1000ms to ensure content loads.

### Content Extraction
- Scrape multiple URLs
- Extract main content
- Remove unwanted elements
- Clean HTML output
- Convert to markdown

### Output Formats
- Markdown (default)
- HTML (processed)
- Raw HTML
- JSON structure
- Screenshots
- Link extraction

### Browser Actions
- Take screenshots
- Scroll pages
- Wait for content
- Click elements
- Full page capture

### Content Filtering
- Include specific tags
- Exclude unwanted tags
- Main content only
- Remove base64 images

## Example Commands

### Basic Scrape
```
/firecrawl scrape https://example.com
```

### Multiple URLs
```
/firecrawl scrape https://site1.com and https://site2.com as markdown
```

### With Screenshot
```
/firecrawl capture https://example.com with full page screenshot
```

### Extract Links
```
/firecrawl get all links from https://example.com
```

### Custom Tags
```
/firecrawl scrape https://example.com including only div, p, h1, h2 tags
```

## Configuration Options

### Output Formats
- `markdown`: Clean markdown content
- `html`: Processed HTML
- `rawHtml`: Original HTML
- `links`: Array of links
- `screenshot`: Base64 image
- `json`: Structured data

### Browser Actions
```json
{
  "type": "screenshot",
  "fullPage": true
}
```

```json
{
  "type": "scroll",
  "direction": "down"
}
```

### Wait Options
- Default: 1000ms
- Custom: specify milliseconds
- Ensures content loads

## Tag Filtering

### Include Tags
- `div`, `p`, `h1`, `h2`
- Article content tags
- Custom selections

### Exclude Tags
- `script`, `style`, `noscript`
- Ads and tracking
- Unwanted elements

## Response Data

### Success Response
- Scraped content
- Metadata (title, language)
- Source URL
- Status code
- File URLs

### Metadata Includes
- Page title
- Language
- Referrer
- Scrape ID
- Status code

## Tips
- Use markdown format for clean text
- Enable screenshots for visual content
- Filter tags for cleaner output
- Set appropriate wait times for dynamic content 