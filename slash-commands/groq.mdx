---
title: "Groq Command Guide"
description: "Learn how to use the Groq slash command for fast LLM inference"
---

## What can you do with it?

Use Groq for high-speed AI inference with various open-source models. Ideal for applications requiring fast response times, high throughput, and efficient processing.

## How to use it?

### Basic Command Structure
```
/groq
prompt: your instructions or questions
```

### Parameters
- **prompt**: Your instructions or questions
- **model** (optional): Specific model to use (defaults to meta-llama/llama-4-maverick-17b-128e-instruct)
- **system prompt** (optional): Override the default system prompt
- **files** (optional): File URLs to include in the request

### Response Format
Returns the model's response in the requested format (JSON by default, or plaintext/markdown/HTML if specified).

## Examples

### Basic Usage
Get a quick response:
```
/groq
prompt: Summarize the key benefits of renewable energy
```

### Advanced Usage
Use a specific model for complex reasoning:
```
/groq
prompt: Solve this complex mathematical problem step by step
model: deepseek-r1-distill-llama-70b
```

### Specific Use Case
Process long documents:
```
/groq
prompt: Analyze this lengthy document and extract key insights
files: [document_url]
model: meta-llama/llama-4-maverick-17b-128e-instruct
```

### Notes
Available models:
- meta-llama/llama-4-maverick-17b-128e-instruct: 131K context, fast (default)
- deepseek-r1-distill-llama-70b: Advanced reasoning
- llama-3.3-70b-versatile: General purpose, high quality
- llama-3.1-8b-instant: Very fast responses
- mixtral-8x7b-32768: Multi-task performance
- gemma2-9b-it: Efficient general tasks

## Key Features

### Model Selection
- Multiple model options
- Varying context lengths
- Different speed tiers
- Specialized capabilities
- Automatic model selection

### File Processing
- Direct file URL handling
- Multi-file support
- No content retrieval needed
- Efficient processing
- Batch operations

### Output Formats
- JSON (default)
- Plain text
- Markdown
- HTML
- Custom formats

## Example Commands

### Basic Query
```
/groq explain quantum computing in simple terms
```

### With Model Selection
```
/groq use deepseek-r1-distill-llama-70b to solve this complex math problem
```

### Process Files
```
/groq analyze these documents [file1.pdf, file2.txt] and compare their content
```

### Custom Output Format
```
/groq format as markdown: create a table comparing these products
```

### Long Context Task
```
/groq use llama-4-maverick-17b with 128k context to analyze this book
```

## Available Models

### Fast & Efficient
- **gemma2-9b-it**: Very fast, 8K context
- **llama-3.1-8b-instant**: Very fast, 131K context
- **llama-4-scout-17b**: Very fast, 16K context

### Balanced Performance
- **llama-3.3-70b-versatile**: Fast, 32K context
- **mixtral-8x7b-32768**: Fast, 32K context
- **llama-4-maverick-17b**: Fast, 131K context (default)

### Advanced Reasoning
- **deepseek-r1-distill-llama-70b**: Complex problem solving, 8K context
- **llama-3.1-70b-versatile**: Long context versatile tasks, 131K context

## Model Selection Guide

Choose based on your needs:
- **Quick responses**: Use instant or scout models
- **Long documents**: Use maverick or llama-3.1 models
- **Complex reasoning**: Use deepseek model
- **General purpose**: Use versatile models
- **Default choice**: llama-4-maverick-17b

## Output Control

### JSON Format (Default)
```
/groq extract product details from this description
```

### Plain Text
```
/groq format as plaintext: write a story about robots
```

### Markdown
```
/groq format as markdown: create documentation for this API
```

### Custom System Prompt
```
/groq with prompt "You are a data scientist" analyze this dataset
```

## Tips
- Default model handles most tasks well
- Use specific models for specialized needs
- JSON output is default for structured data
- Include file URLs directly in commands
- Specify format explicitly when needed 