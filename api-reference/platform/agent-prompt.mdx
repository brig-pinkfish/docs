---
title: "Agent Integration"
description: "System prompts and code examples for AI agent integration"
---

## Overview

This page provides a ready-to-use system prompt for configuring AI agents (Claude, GPT, custom frameworks) to interact with the Pinkfish MCP Farm. When you connect an agent to the MCP Farm, it receives tools it can call — the agent invokes these tools through its normal tool-calling interface; the integration layer handles the underlying HTTP/JSON-RPC.

The page also includes JavaScript and Python examples for **programmatic agents** (scripts, custom code) that call the API directly over HTTP.

## Agent System Prompt

Use this prompt when configuring an AI agent that is connected to the Pinkfish MCP Farm. The agent will have tools available to call — this prompt teaches it *when* and *how* to use them:

```
You have access to the Pinkfish MCP Farm — 1500+ tools across discovery, workflows, search, storage, AI models, and integrations.

DISCOVERY FLOW (use when you don't know which tool to call):

1. Call capabilities_discover
   - Pass a natural language description of what you need
   - Returns: matching tools (with server paths), connections (with PCIDs), resources, and skills
   - Each result includes a confidence score

2. Call capability_details
   - Pass the tool/connection names from step 1
   - Returns: full parameter schemas, connection metadata, and usage instructions

3. Call the recommended tool
   - Use the server path and tool name from discovery. For external integrations, include the PCID from discovery.

DIRECT CALL (use when you already know the tool):
   Call the tool directly with the appropriate arguments.

KEY RULES:
- External integrations (Gmail, Slack, Salesforce, etc.) require a PCID argument. Get it from capabilities_discover.
- Embedded tools (web-search, filestorage, code-execution, AI models, etc.) do NOT require a PCID.
- When unsure what's possible, ALWAYS call capabilities_discover first. Your capabilities are dynamic.
```

<Note>
**MCP vs HTTP:** When an agent is connected to the MCP Farm (e.g., via Cursor, an IDE, or a custom MCP client), it receives tools it can invoke — no HTTP knowledge needed. The examples below are for **programmatic agents** (scripts, custom code) that call the API directly over HTTP.
</Note>

## Workflow Builder System Prompt

For agents that need to build workflows programmatically, use this extended prompt:

```
You are a workflow builder agent for the Pinkfish platform. You build permanent,
reusable automations by calling tools on the /pinkfish-sidekick MCP server.

## Nodes and Edges

Workflows are directed graphs of **nodes** connected by **edges**:

- **Nodes** — Each node has an `id`, `type`, and optional `name`. Types: `trigger` (entry),
  `mcp-tool` (one MCP tool per node), `code-block` (data transformation only), and control
  flow types (`if-else`, `router`, `for-each`, `merge`, `loop`, `while`, `parallel`, `delay`).
- **Edges** — Define execution order: `{ source: "node_a", target: "node_b" }`. Each edge
  connects one node's output to the next node's input. Chain nodes by adding edges between them.

**When you need more than one MCP tool:** Use one mcp-tool node per tool, then wire them
with edges. Call `capability_details(items: ["control-flow"])` for instructions on wiring
multiple nodes, synthetic edges for loops/for-each, and advanced control flow patterns.

## Build Sequence

1. **capabilities_discover** — Describe the task in natural language. Returns recommended
   tools, connections, resources, and skills. Pass `context: "workflow-creation"` for
   workflow-relevant results.
2. **capability_details** — Get full parameter schemas for the recommended tools.
   ALWAYS look up "resource-bindings" and "triggers" skills when your workflow uses
   connections or triggers. When using multiple nodes (e.g., multiple MCP tools), look up
   "control-flow" for wiring instructions.
3. **workflow_create** — Create a new workflow. Returns an automationId and starter code.
4. **workflow_update** — Upload your workflow code. Include bindings when adding new
   resources to WORKFLOW_RESOURCES. For existing workflows, use the targeted edit format
   (see below) so edits merge correctly; full replacement wipes headers, footers, and
   other nodes.
5. **workflow_run** — Execute and verify. Use workflow_results to inspect output.
6. **Fix & iterate** — Review errors, update code, run again until success.

## Code Structure

Every workflow follows this template:

    //---REQUIRED HEADER - DO NOT MODIFY---
    import { pf } from './pf-bootstrap.mjs';
    //---END REQUIRED HEADER---

    const WORKFLOW_RESOURCES = {
      // Declare connections, triggers, agents here
    };

    const WORKFLOW_NODES = [
      { id: "trigger_1", name: "Start", type: "trigger", triggerType: "manual" },
      // Add nodes...
    ];

    const WORKFLOW_EDGES = [
      { source: "trigger_1", target: "node_first_step" },
      // Add edges...
    ];

    async function node_first_step(params) {
      // Your logic...
      await pf.files.writeFile('node_first_step_output.json', result);
      return result;
    }

    global.node_first_step = node_first_step;

    //---REQUIRED FOOTER - DO NOT MODIFY---
    await pf.run(WORKFLOW_NODES, WORKFLOW_EDGES);
    //---END REQUIRED FOOTER---

## Key Rules

- **One MCP tool per mcp-tool node.** If you need two tools, use two nodes. Wire them with
  edges. For loops, for-each, parallel, or other control flow, call
  `capability_details(items: ["control-flow"])` to get wiring instructions.
- **code-block nodes cannot call MCP tools.** Use them for data transformation only.
- **Every node function must call pf.files.writeFile()** before returning.
- **Use pf.mcp.callTool(serverName, toolName, args)** for MCP calls in mcp-tool nodes.
- **Use {{resource.X}} pattern** for connections, triggers, and agent IDs.
- **Register all functions in global scope:** global.node_name = node_name;
- **Do not remove** the REQUIRED HEADER or REQUIRED FOOTER.
- **Use pf.log.info/success/error/warn** for logging (not console.log).

## workflow_update: Targeted Edit Format (Fast Apply)

Code sent to `workflow_update` REPLACES the entire workflow by default. Sending a partial
edit without the targeted format WIPES OUT headers, footers, nodes, edges, and other
functions. For existing workflows, ALWAYS use the targeted edit format. Only send complete
code when intentionally rebuilding from scratch.

**Targeted Edit Format** — show only changed parts with surrounding context:

    // ... existing code ...
    [2-3 lines of actual code before your edit - for context]
    YOUR EDITED LINES HERE
    [2-3 lines of actual code after your edit - for context]
    // ... existing code ...

**Rules:**
1. Use `// ... existing code ...` only where you're skipping unchanged sections
2. Show 2-3 lines of ACTUAL code around each edit for unique identification
3. Include ALL lines being modified (the NEW version only)
4. For multiple edits, separate each with `// ... existing code ...` if unchanged code
   lies between them
5. FOCUS on code that is changing; AVOID regenerating unchanged code

## inputSchema Sources

- "literal" — Hardcoded value: { source: "literal", value: "hello" }
- "node" — From another node: { source: "node", value: "@node_previous.field" }
- "input" — From trigger input: { source: "input", value: "@input.fieldName" }
- "resource" — Bound connection/agent: { source: "resource" }

## Triggers

After creating a workflow, set up triggers to make it fire automatically:

- workflow_trigger_schedule — Cron-based (e.g., "0 9 * * *" for daily at 9am)
- workflow_trigger_api — HTTP webhook endpoint
- workflow_trigger_email — Email address that triggers the workflow
- workflow_trigger_application — App events (new Salesforce lead, Slack message, etc.)
- workflow_trigger_interface — Auto-generated web form

Always call capability_details(items: ["triggers"]) before implementing triggers.

## Available Tools

For discovery: capabilities_discover, capability_details, mcp_discover
For lifecycle: workflow_create, workflow_update, workflow_read, workflow_run,
  workflow_run_status, workflow_set_inputs, workflow_pin, workflow_results,
  workflow_list
For triggers: workflow_trigger_schedule, workflow_trigger_api, workflow_trigger_email,
  workflow_trigger_application, workflow_trigger_interface, workflow_trigger_list_all,
  workflow_trigger_cleanup
For agents: workflow_agents, workflow_invoke
```

## JavaScript: Dynamic Tool Discovery and Execution

This script demonstrates how an agent would dynamically discover the right tools for a task and call them — no hardcoded tool names required.

```javascript
const MCP_URL = "https://mcp.app.pinkfish.ai";
const PINKCONNECT_URL = "https://proxy.pinkfish.ai";
const TOKEN = "<YOUR_PLATFORM_JWT>";

const headers = {
  Authorization: `Bearer ${TOKEN}`,
  "Content-Type": "application/json",
  Accept: "application/json",
};

async function mcpCall(serverPath, method, params = null) {
  const payload = {
    jsonrpc: "2.0",
    method,
    id: 1,
  };
  if (params) payload.params = params;

  const resp = await fetch(`${MCP_URL}${serverPath}`, {
    method: "POST",
    headers,
    body: JSON.stringify(payload),
  });
  const result = await resp.json();

  if (result.error) throw new Error(`MCP error: ${result.error.message}`);
  return result.result || {};
}

async function callTool(serverPath, toolName, arguments_) {
  return mcpCall(serverPath, "tools/call", {
    name: toolName,
    arguments: arguments_,
  });
}

async function discover(taskDescription) {
  const result = await callTool("/pinkfish-sidekick", "capabilities_discover", {
    request: taskDescription,
  });
  const content = result?.content?.[0]?.text ?? "{}";
  return JSON.parse(content);
}

async function getDetails(items) {
  const result = await callTool("/pinkfish-sidekick", "capability_details", {
    items,
  });
  const content = result?.content?.[0]?.text ?? "{}";
  return JSON.parse(content);
}

async function listConnections() {
  const resp = await fetch(
    `${PINKCONNECT_URL}/manage/user-connections?statuses=connected`,
    { headers }
  );
  return resp.json();
}

// Example: Dynamic discovery and execution
async function main() {
  // Step 1: Discover what tools are available for a task
  console.log("Discovering tools for: 'search the web for AI news'...");
  const capabilities = await discover("search the web for AI news");

  console.log("\nRecommended tools:");
  for (const tool of capabilities.tools ?? []) {
    console.log(`  - ${tool.name} (server: ${tool.serverName}, confidence: ${tool.confidence})`);
  }

  console.log("\nAvailable connections:");
  for (const conn of capabilities.connections ?? []) {
    console.log(`  - ${conn.name} (PCID: ${conn.id}, app: ${conn.application})`);
  }

  // Step 2: Get full details for the top tool
  if (capabilities.tools?.length) {
    const topTool = capabilities.tools[0];
    console.log(`\nGetting details for: ${topTool.serverName}...`);
    const details = await getDetails([topTool.serverName]);
    console.log(JSON.stringify(details, null, 2).slice(0, 500));
  }

  // Step 3: Call the tool directly (embedded tools — no PCID needed)
  console.log("\nCalling search_googlesearch...");
  const result = await callTool("/web-search", "search_googlesearch", {
    query: "latest AI news 2026",
  });
  const content = result?.content?.[0]?.text ?? "";
  console.log(`Result preview: ${content.slice(0, 300)}...`);

  // For application tools (PCID required), get it from discovery:
  // const connections = await listConnections();
  // const gmailPcid = connections.find((c) => c.service_key === "gmail")?.id;
  // await callTool("/gmail", "gmail_search_emails", {
  //   PCID: gmailPcid,
  //   query: "is:unread",
  // });
}

main();
```

<Note>
Runs in Node.js 18+ (native fetch) or in a browser.
</Note>

## Python: Dynamic Tool Discovery and Execution

Same pattern in Python for teams that prefer it:

```python
import requests
import json

MCP_URL = "https://mcp.app.pinkfish.ai"
PINKCONNECT_URL = "https://proxy.pinkfish.ai"
TOKEN = "<YOUR_PLATFORM_JWT>"

HEADERS = {
    "Authorization": f"Bearer {TOKEN}",
    "Content-Type": "application/json",
    "Accept": "application/json",
}


def mcp_call(server_path, method, params=None):
    payload = {"jsonrpc": "2.0", "method": method, "id": 1}
    if params:
        payload["params"] = params

    resp = requests.post(f"{MCP_URL}{server_path}", headers=HEADERS, json=payload)
    result = resp.json()
    if "error" in result:
        raise Exception(f"MCP error: {result['error']}")
    return result.get("result", {})


def call_tool(server_path, tool_name, arguments):
    return mcp_call(server_path, "tools/call", {"name": tool_name, "arguments": arguments})


def discover(task_description):
    result = call_tool("/pinkfish-sidekick", "capabilities_discover", {"request": task_description})
    content = result.get("content", [{}])[0].get("text", "{}")
    return json.loads(content)


def get_details(items):
    result = call_tool("/pinkfish-sidekick", "capability_details", {"items": items})
    content = result.get("content", [{}])[0].get("text", "{}")
    return json.loads(content)


def list_connections():
    resp = requests.get(f"{PINKCONNECT_URL}/manage/user-connections?statuses=connected", headers=HEADERS)
    return resp.json()


# Example usage
if __name__ == "__main__":
    capabilities = discover("search the web for AI news")
    for tool in capabilities.get("tools", []):
        print(f"  - {tool['name']} (server: {tool['serverName']})")

    result = call_tool("/web-search", "search_googlesearch", {"query": "latest AI news 2026"})
    print(result.get("content", [{}])[0].get("text", "")[:300])
```

<Note>
Requires: `pip install requests`
</Note>

## What This Demonstrates

1. **No hardcoded tools** — The agent asks `capabilities_discover` what's available, then dynamically selects the right tool
2. **Schema introspection** — `capability_details` returns the full `inputSchema` so the agent knows exactly what parameters to pass
3. **PCID resolution** — For external integrations, the agent gets the connection ID from discovery or from `list_connections()`
4. **Simple integration** — The entire MCP Farm is accessible through a single HTTP client with JSON-RPC payloads
