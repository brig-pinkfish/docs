---
title: "Breaking Big Jobs into Smaller Tasks"
description: "Learn how to use the Queue service to handle complex workflows that might timeout or become unwieldy"
---

## The Problem: When Your Job is Too Big

Have you tried to create a workflow that needs to process 1000 customer records, send 100 page PDF to the LLM, generate 50 reports, or handle a massive dataset? You might run into these issues:

- **Timeouts**: Workflows can only run for 10 minutes total
- **Complexity**: Large workflows become hard to debug and maintain
- **Failure Risk**: If step 47 out of 50 fails, you lose all your progress
- **Resource Limits**: Processing everything at once can overwhelm systems such as LLMs that can quickly hit context window limits with large amounts of data

## The Solution: The Producer-Consumer Pattern

Think of it like a factory assembly line with three specialized stations:

1. **Producer** (Station 1): Receives the big task and breaks it into individual items in a single batch
2. **Consumer** (Station 2): Processes each batch item one at a time
3. **Aggregator** (Station 3): Collects all the completed batch items and creates the final result

Using Pinkfish's Queue service, you can orchestrate these three workflows to handle any size batch efficiently.

## How it works

### Workflow 1: Producer

**Purpose**: Break down your big job into individual items in a single batch

This workflow:

- Receives your initial request (e.g., "process 500 customers")
- Uses the Batch service to create a batch with unique batchId
- Breaks the work into individual batch items
- Batch service places all items into a Queue to be processed
- Assigns the Consumer to process each batch item

### Workflow 2: Consumer

**Purpose**: Process individual batch items and store results

This workflow:

- Gets triggered for each batch item
- Processes one batch item at a time (e.g., one customer record)
- Stores results in datastore or filestore with batchId metadata
- Marks the batch item as complete
- Handles retries automatically if something fails

### Workflow 3: Aggregator

**Purpose**: Compile final results when all batch items are done

This workflow:

- Gets triggered automatically when the batch completes
- Retrieves all batch results from datastore/filestore using the batchId
- Compiles the final output (reports, summaries, etc.)
- Sends notifications or delivers final results
- Cleans up temporary data if needed

## Benefits of the Producer-Consumer Pattern

### üîÑ **Reliability**

- Each workflow is simple and focused
- Failed items retry automatically
- Batch tracking shows exactly what's complete
- No risk of losing all progress

### üêõ **Easier Debugging**

- Test each workflow independently
- Clear separation of concerns
- Detailed queue monitoring
- Easy to identify bottlenecks

### ‚ö° **Better Performance**

- Consumers run in parallel
- Automatic concurrency management
- No resource bottlenecks
- Scales to handle any volume

### üìä **Progress Tracking**

- Real-time batch monitoring
- See exactly which items are done
- Queue dashboard shows progress
- Automatic retry handling

### üß† **LLM Optimization**

- Works within LLM context window limits
- Processes large datasets by breaking them into manageable chunks
- Avoids overwhelming AI models with too much data at once
- Enables consistent processing quality across all items

## Best Practices

- **Start Small**: Test with a few batch items before scaling
- **Monitor Performance**: Use queue dashboard to optimize concurrency
- **Handle Failures**: Design consumer to be fault-tolerant
- **Store Results**: Use datastore for metadata, filestore for large outputs (always include batchId for easy aggregation)
- **Clean Up**: Use aggregator to archive or delete temporary data

The Producer-Consumer pattern transforms complex, risky workflows into reliable, scalable systems. By breaking big jobs into three focused workflows, you get better performance, easier debugging, and complete visibility into your automation processes.

---

**Note**: This pattern is also known as the **Work Queue Pattern**, **Batch Processing Pattern**, or **Fan-out/Fan-in Pattern** in computer science.

---

**Questions?** Reach out on Discord https://discord.com/invite/HaDg7R4VZG
