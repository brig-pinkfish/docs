---
title: "Using Datastore as a Job Queue"
description: "Learn how to implement a job queue system using the datastore command"
---

## Overview

The datastore can be used as an effective job queue system by leveraging its key-sort field structure, metadata capabilities, and trigger functionality. This guide explains how to create, manage, and process jobs using the datastore as a queue.

## Basic Job Queue Concept

A job queue in the datastore works by:
1. Creating items that represent jobs with specific metadata and triggers
2. Using metadata to track job state (`not_started`, `started`)
3. Updating the job's status to trigger automation
4. Having your automation process the job when triggered

Key benefits of using datastore as a job queue:
* **Organized Structure**: Group similar jobs under the same key
* **Flexible Metadata**: Store job parameters and status
* **Built-in Triggers**: Automatically start jobs when status changes
* **Status Tracking**: Monitor job progress and completion
* **Silent Updates**: Option to update status without triggering automation

## Datastore Triggers Feature

Datastore items can have triggers attached that fire when the item is updated. In a job queue system:

1. **Trigger Attachment**: When creating a job item, you attach a trigger that specifies:
   * What action to take (e.g., "generate_report")
   * When to execute (e.g., when status changes from `not_started` to `started`)

2. **Trigger Execution**: When the item is updated, your automation receives the job data:
```json
{
  dataChanged: { content: 'My changed content' },
  newDataset: { content: 'My changed content', metadata: 'my metadata' }
}
```

3. **Trigger Control**: You can control trigger execution:
   * Normal updates fire triggers
   * Updates with "trigger off" bypass trigger execution

Your automation code can use this payload to:
* Check what data changed
* Verify the status transition
* Access the job parameters

## Job States

Common job states:
* `not_started`: Initial state when job is created
* `started`: Job is currently running
* `completed`: Job finished successfully
* `failed`: Job encountered an error

## Creating Jobs

### Basic Job Creation
```
/datastore create a new item with:
key: jobs
sortField: job-2024-03-15-001
content: Process monthly report
metadata: {
    "status": "not_started",
    "type": "report_generation",
    "parameters": {
        "report_type": "monthly",
        "department": "sales"
    }
}
trigger: {
    "type": "status_change",
    "action": "generate_report",
    "condition": "getPreviousValue('metadata.status') === 'not_started' && metadata.status === 'started'"
}
```

## Managing Jobs

### Updating Jobs

There are two ways to update jobs:

1. **Normal Update (Triggers Fire)**
```
/datastore update item with:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "started",
    "started_at": "2024-03-15T10:05:00Z"
}
```

2. **Silent Update (No Triggers Fire)**
```
/datastore update item with trigger off:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "completed",
    "completed_at": "2024-03-15T10:15:00Z",
    "result": {
        "success": true,
        "processed_items": 1000
    }
}
```

This second option is particularly useful for:
* Updating job status from your worker
* Recording completion or failure states
* Updating job metadata without triggering automation
* Adding results or progress information

### Real-World Examples

1. **Starting a Data Processing Job (With Trigger)**
```
/datastore update item with:
key: jobs-data-processing
sortField: data-proc-2024-03-15-001
metadata: {
    "status": "started",
    "started_at": "2024-03-15T10:00:00Z"
}
```

2. **Recording Job Completion (Without Trigger)**
```
/datastore update item with trigger off:
key: jobs-data-processing
sortField: data-proc-2024-03-15-001
metadata: {
    "status": "completed",
    "completed_at": "2024-03-15T10:15:00Z",
    "result": {
        "processed_records": 1000,
        "success": true
    }
}
```

## Best Practices

1. **Trigger Management**
   * Use normal updates to start jobs (trigger fires)
   * Use "trigger off" updates for status changes from your worker
   * Consider using "trigger off" for progress updates

2. **Status Updates**
   * Start jobs with normal updates to trigger automation
   * Record completion with "trigger off" updates
   * Use "trigger off" for any auxiliary status information

3. **Error Handling**
   * Record errors using "trigger off" updates
   * Include detailed error information for debugging

## Example: Complete Job Flow

1. **Create Initial Job**
```
/datastore create a new item with:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "not_started",
    "type": "report_generation"
}
trigger: {
    "type": "status_change",
    "action": "generate_report",
    "condition": "getPreviousValue('metadata.status') === 'not_started' && metadata.status === 'started'"
}
```

2. **Start the Job (Triggers Fire)**
```
/datastore update item with:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "started",
    "started_at": "2024-03-15T10:05:00Z"
}
```

3. **Record Progress (No Trigger)**
```
/datastore update item with trigger off:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "started",
    "progress": 75,
    "last_updated": "2024-03-15T10:10:00Z"
}
```

4. **Record Completion (No Trigger)**
```
/datastore update item with trigger off:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "completed",
    "completed_at": "2024-03-15T10:15:00Z",
    "result": {
        "success": true,
        "processed_items": 1000
    }
}
```

This approach gives you fine-grained control over when triggers fire, allowing you to update job status and metadata without causing unintended automation runs.