---
title: "Using Datastore as a Job Queue"
description: "Learn how to implement a job queue system using the datastore command"
---

## Overview

The datastore can be used as a job queue system. This guide explains how to create, manage, and process jobs using the datastore as a queue.

## Basic Job Queue Concept

A job queue in the datastore works by:

1. Creating items that represent jobs with specific metadata and triggers

2. Using metadata to track job state (`not_started`, `started`)

3. Updating the job's status to trigger an automation

4. Having your automation process the job when triggered

Key benefits of using datastore as a job queue:

* **Organized Structure**: Group similar jobs under the same key

* **Flexible Metadata**: Store job parameters and status

* **Built-in Triggers**: Automatically start jobs when status changes

* **Status Tracking**: Monitor job progress and completion

* **Silent Updates**: Option to update status without triggering automation

## Datastore Triggers Feature

Datastore items can have one or more triggers attached that fire *when the item is updated*. This feature is not specific to the job queue - it's just how the datastore feature works. Which makes it very handy for setting up job queue system:

1. **Attach Trigger**: When creating a job item, attach a trigger (see Triggers guide)

2. **Trigger Execution**: When the item is updated, the trigger fires and your automation receives the contents of the data store item as shown below.&#x20;

3. **Triggers On and Off**: You can control if an update to a datastore item fires the trigger

   * Any update to a datastore item will fire all attached triggers

   * However, you can request "trigger off" to bypass trigger execution. Here's a sample prompt: ` update the status of my datastore item to "complete" where the item has key:jobs and sortField:onboarding-02-24-25 with triggers off`

```json Payload sent to an automation via a trigger onChange
{
  dataChanged: { content: 'My changed content' },
  newDataset: { content: 'My changed content', metadata: 'existing metadata' }
}
```

Your automation code can use this payload to:

* Check what data changed

* Verify the status transition

* Access the job parameters

## Job Statuses

Common job statuses that you may want to use:

* `not_started`: Initial state when job is created

* `started`: Set to started to fire triggers

* `completed`: Job finished successfully (update with triggers off)

* `failed`: Job encountered an error (update with triggers off)

## Creating Jobs

### Basic Job Creation

```
/datastore create a new item with:
key: jobs
sortField: job-2024-03-15-001
content: Process monthly report
metadata: {
    "status": "not_started",
    "type": "report_generation",
    "parameters": {
        "report_type": "monthly",
        "department": "sales"
    }
},
"triggerUrls": {
    "URL1": "APIKEY1"
}
```

## Managing Jobs

### Updating Jobs

There are two ways to update jobs:

1. **Normal Update (Triggers Fire)**

```
/datastore update item with:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "started",
    "started_at": "2024-03-15T10:05:00Z"
}
```

1. **Silent Update (No Triggers Fire)**

```
/datastore update item with triggers off:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "completed",
    "completed_at": "2024-03-15T10:15:00Z",
    "result": {
        "success": true,
        "processed_items": 1000
    }
}
```

This second option is particularly useful for:

* Updating job status from your worker

* Recording completion or failure states

* Updating job metadata without triggering automation

* Adding results or progress information

### Real-World Examples

1. **Starting a Data Processing Job (With Trigger)**

```
/datastore update item with:
key: jobs-data-processing
sortField: data-proc-2024-03-15-001
metadata: {
    "status": "started",
    "started_at": "2024-03-15T10:00:00Z"
}
```

1. **Recording Job Completion (Without Trigger)**

```
/datastore update item with trigger off:
key: jobs-data-processing
sortField: data-proc-2024-03-15-001
metadata: {
    "status": "completed",
    "completed_at": "2024-03-15T10:15:00Z",
    "result": {
        "processed_records": 1000,
        "success": true
    }
}
```

## Best Practices

1. **Trigger Management**

   * Use normal updates to start jobs (trigger fires)

   * Use "trigger off" updates for status changes from your worker

   * Consider using "trigger off" for progress updates

2. **Status Updates**

   * Start jobs with normal updates to trigger automation

   * Record completion with "trigger off" updates

   * Use "trigger off" for any auxiliary status information

3. **Error Handling**

   * Record errors using "trigger off" updates

   * Include detailed error information for debugging

## Example: Complete Job Flow

1. **Create Initial Job**

```
/datastore create a new item with:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "not_started",
    "type": "report_generation"
},
"triggerUrls": {
    "URL1": "APIKEY1"
}
```

1. **Start the Job (Triggers Fire)**

```
/datastore update item with:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "started",
    "started_at": "2024-03-15T10:05:00Z"
}
```

1. **Record Progress (No Trigger)**

```
/datastore update item with trigger off:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "started",
    "progress": 75,
    "last_updated": "2024-03-15T10:10:00Z"
}
```

1. **Record Completion (No Trigger)**

```
/datastore update item with trigger off:
key: jobs
sortField: job-2024-03-15-001
metadata: {
    "status": "completed",
    "completed_at": "2024-03-15T10:15:00Z",
    "result": {
        "success": true,
        "processed_items": 1000
    }
}
```

This approach gives you fine-grained control over when triggers fire, allowing you to update job status and metadata without causing unintended automation runs.